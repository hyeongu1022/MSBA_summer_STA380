---
title: "STA final"
author: Hyeon Gu Kim, Shakirah Oladokun, Jazline Keli
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.duplicate.label = "allow")
knitr::opts_chunk$set(fig.path='Figs/')
#tinytex::install_tinytex()
tinytex:::install_prebuilt()
```

## Visual story telling part 1: green buildings

### Understanding of the Dataset

In this section I will explore and learn about data presented in greenbuildings.csv

Let see how many buildings in the dataset total and how many of them are green buildings.

```{r }
library(rafalib)
library(ggplot2)
## Read CSV file
blds = read.csv("/Users/macintosh/Documents/R studio/greenbuildings.csv") 

## Calculate total number of buildings
num_blds = nrow(blds)
prtst = paste0("Number of buildings in greenbuildings.csv = ", num_blds)
print(prtst)

## Calculating green buildings
num_green_blds = sum(blds$LEED)+sum(blds$Energystar)
prtst = paste0("Number of green buildings = ", num_green_blds)
print(prtst)


```

## Data cleaning

I noticed that a handful of the buildings in the data set had very low occupancy rates (less than 10% of available space occupied). I decided to remove these buildings from consideration, on the theory that these buildings might have something weird going on with them, and could potentially distort the analysis.

Let's find buildings with occupancy rate less than 10%

```{r  }

## 
num_low_occup_blds = sum(blds$leasing_rate <= 10)

low_occup_blds_pct = (num_low_occup_blds/num_blds)*100
low_occup_blds_pct = round(low_occup_blds_pct, 2)

prtst = paste0("The number of buildings with occupancy rates <= 10% is ", num_low_occup_blds, " and that constitues only ",low_occup_blds_pct,"% of all buidings" )

print(prtst)

```
The graph below shows that there are some buildings that are below 10% occupacy.
```{r}
ggplot(data = blds) + 
  geom_point(mapping = aes(x = age, y = leasing_rate, color = age), size=1) + geom_hline(yintercept=10, linetype='dashed', color='red', size=1) 
```
Let's remove low-occupancy buildings from the dataset

```{r}

blds = subset(blds, blds$leasing_rate > 10) 
num_blds = nrow(blds)
prtst = paste0(num_blds)
print(prtst)


```
## Create two datasets - one for green buildings and another one for non-green buildings

```{r}

green_blds = subset(blds, (blds$LEED | blds$Energystar )) 
nrow(green_blds)

non_green_blds = subset(blds, (!blds$LEED & !blds$Energystar )) 
nrow(non_green_blds)

nrow(green_blds) + nrow(non_green_blds)


```
Let's check developer's median prices for non-green and green buildings

```{r }

med_rent_non_green = median(non_green_blds$Rent)
print(med_rent_non_green)

med_rent_green = median(green_blds$Rent)
print(med_rent_green)

diff_median_rent = med_rent_green - med_rent_non_green
print(diff_median_rent)

```
They look to be correct
========================

## Rent Distribution Characteristics

Now let's look at different distribution characteristics for the Rent

We start with drawing Box plots.

A box plot (or “box-and-whisker plot”) is an alternative to a histogram to give a quick visual display of the main features of a set of data.

```{r  }

mypar(1,2)

boxplot(non_green_blds$Rent, data=non_green_blds, ylab="Rent", main="Non-Green buildings")

boxplot(green_blds$Rent, data=green_blds, ylab="Rent", main="Green buildings")

```

Now let's look at Rent histograms

```{r }

mypar(1,2)

smallest <- floor( min(non_green_blds$Rent) )
largest <- ceiling( max(non_green_blds$Rent) )
bins <- seq(smallest, largest)

hist(non_green_blds$Rent, breaks=bins, xlab="Rent", main="Non-Green buildings")

smallest <- floor( min(green_blds$Rent) )
largest <- ceiling( max(green_blds$Rent) )
bins <- seq(smallest, largest)

hist(green_blds$Rent, breaks=bins, xlab="Rent", main="Green buildings")

```

We still see some outliers above "upper whisker" that could potentially distort the analysis.

So let's cleanup both datasets by removing building with rents above "upper whisker", recreate box plots and histograms and then can use mean for Rent. 

We can find "lower whisker" and "upper whisker" values using boxplot.stats().
The "lower whisker" is the first number and the "upper whisker" is the last (5th) number in $stats

```{r }

boxplot.stats(non_green_blds$Rent, do.conf = FALSE, do.out = FALSE)

```

```{r  }

boxplot.stats(green_blds$Rent, do.conf = FALSE, do.out = FALSE)

```


```{r  }
non_green_blds_clean = subset(non_green_blds, non_green_blds$Rent < 56.27)
green_blds_clean = subset(green_blds, green_blds$Rent < 55.94)

```

Now let's redraw box plots

```{r  }

mypar(1,2)

boxplot(non_green_blds_clean$Rent, data=non_green_blds_clean, ylab="Rent", main="Non-Green buildings")

boxplot(green_blds_clean$Rent, data=green_blds_clean, ylab="Rent", main="Green buildings")

```

Now let's look at Rent histograms after cleaning

```{r }

mypar(1,2)

smallest <- floor( min(non_green_blds_clean$Rent) )
largest <- ceiling( max(non_green_blds_clean$Rent) )
bins <- seq(smallest, largest)

hist(non_green_blds_clean$Rent, breaks=bins, xlab="Rent", main="Non-Green buildings")

smallest <- floor( min(green_blds_clean$Rent) )
largest <- ceiling( max(green_blds_clean$Rent) )
bins <- seq(smallest, largest)

hist(green_blds_clean$Rent, breaks=bins, xlab="Rent", main="Green buildings")

```

Those histograms look close to normal distribution. So we can use mean to compare rental prices

```{r}

mean_rent_non_green = mean (non_green_blds_clean$Rent)
print(mean_rent_non_green)

mean_rent_green = mean(green_blds_clean$Rent)
print(mean_rent_green)

diff_mean_rent = mean_rent_green - mean_rent_non_green
print(diff_mean_rent)
 
```
Now we can see that more accurate analysis results in less price difference between green and non-green rents which is $\$1.85$ vs. $\$2.60$

#REPORT GREENBUILDINGS
Firstly we decided to re-explore and learn about the data by calculating the total number of buildings and the total number of green buildings, then decided to do some data cleaning by removing the buildings with occupancy rate less than 10%. The amount of total buildings reduced by 215, from 7,894 to 7,679.
After that we proceeded to create two subsets, one with the green buildings and one with non-green buildings and then calculated the median of those different subsets.
We then wanted to look at the distribution characteristics for the rent starting with Box plot and then Histograms. With the box plots we saw that there were some outliers above "upper whisker" that could potentially distort the analysis, so we decided to clean it up by removing building with rents above "upper whisker”. For the histogram it was very much negatively skewed, so we decided to recreate both.
We used boxplot. Stats() to measure the upper whiskers , subset the data into the two groups and measure them to be less than the highest upper whisker value, this gave us a new clean data set for both green buildings and non-green buildings.
After that we performed analysis with the box plot which appeared with less outliers and the histogram was also more normally distributed.
The final step was to recalculate the mean of the different buildings and we concluded that we cannot agree with the conclusions of her on-staff stats gurus. The results found from this analysis was $1.85 more which was lower than $2.60 they had calculated, meaning the additional revenue is also not coherent. It stills seems like a good financial move even though it will take longer to recuperate the costs (10.8 years).

## Visual story telling part 2: flights at ABIA

### Exploring flights dataset
We checked the frequency of flights by month. Looks like June has the highest frequency but the overall frequencies are well-distributed across every months.
```{r}
library(rafalib)
library(lubridate)
library(tidyverse)
## Read CSV file
flights = read.csv("/Users/macintosh/Documents/R studio/ABIA.csv")
#airports = read.csv("/Users/macintosh/Documents/R studio/airport-codes.csv")
num_flights = nrow(flights)
ggplot(flights, aes(Month)) + geom_bar() + geom_text(stat='count', aes(label=..count..), vjust=-1) + ylim(0, 15000) +
  scale_x_discrete(name ="Months", limits=c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12")) +
  ggtitle("Flight Frequency by Month")
```
We decided to explore why the flights were canceled. Notice that there are comparably very small number of cancelled flights.
```{r}
numCancel <- nrow(flights[which(flights$Cancelled == 1),])
numNotCancel <- nrow(flights[which(flights$Cancelled == 0),])

ggplot(data = flights) + 
  geom_bar(mapping = aes(x=CancellationCode, y=Distance), stat='identity') + 
  facet_wrap(~Cancelled) +
  labs(title="Cancelled (1) vs Not Cancelled(0)")

```
Now let's dig in the reason of canceled flights. Looks like the most of the reason for cancellation is due to carrier. Note that there is zero security reason for cancellation. 
```{r}
flights[which(flights$Cancelled == 1),]

numA <- nrow(flights[which(flights$CancellationCode=="A"), ])
numB <- nrow(flights[which(flights$CancellationCode=="B"), ])
numC <- nrow(flights[which(flights$CancellationCode=="C"), ])
numD <- nrow(flights[which(flights$CancellationCode=="D"), ])

numCancel <- c(numA, numB, numC, numD)

cancel_code_count = data.frame("A"=numA, "B"=numB, "C"=numC, "D"=numD)
#cancel_code_count = t(cancel_code_count)

barplot(numCancel, main = "Number of Cancellation Codes",
xlab = "Cancellation Codes",
ylab = "Counts",
names.arg = c("A", "B", "C", "D"),
col = "darkred")
```
Here, we can observe that most of cancellation had comparably short distances. However, the cancellation code "C", which is the cancellation due to NAS, had one big spike when the distance is near 1000. This indicates that in year 2008, there were lots of flight cancellation due to the national aviation system (NAS) that refer to a broad set of conditions. 
```{r}
ggplot(data=flights) + 
  geom_histogram(aes(x=Distance, stat(density)), binwidth=2) + 
  facet_grid(CancellationCode~.) +
  theme_bw(base_size=18) 
```

## Portfolio Modeling

### Portfolio 1
For this first portfolio, we chose the following ETFs:
JPEM = JPMorgan Diversified Return Emerging Markets Equity ETF
VGT = Vanguard Information Technology ETF
SCHZ = Schwab U.S. Aggregate Bond ETF
XOM = iShares J.P. Morgan USD Emerging Markets Bond ETF
USO = United States Oil Fund

And we loaded these stocks' data sets from January 9th, 2015. Then we adjusted Open, High, Low, Close prices for splits and dividends. We combined all the data matrices into one for close-to-close returns. From this matrix, we could get overall view of the each stocks' returns of each days. The plots below indicates close positive correlation between the close-to-close returns of JPEM and that of VGT, JPEM and XOM, and VGT and XOM. 
```{r}
library(mosaic)
library(quantmod)
library(foreach)

# Import a few stocks
# JPEM = JPMorgan Diversified Return Emerging Markets Equity ETF
# VGT = Vanguard Information Technology ETF
# SCHZ = Schwab U.S. Aggregate Bond ETF
# XOM = iShares J.P. Morgan USD Emerging Markets Bond ETF
# USO = United States Oil Fund
mystocks = c("JPEM", "VGT", "SCHZ", "XOM", "USO")
getSymbols(mystocks, from = "2015-01-09")

for(ticker in mystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

head(JPEMa)

# Combine all the returns in a matrix
all_returns = cbind(	ClCl(JPEMa),
								ClCl(VGTa),
								ClCl(SCHZa),
								ClCl(XOMa),
								ClCl(USOa))
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))

# Compute the returns from the closing prices
pairs(all_returns)
```
We tried the bootstrap resampling on one random day just as a small simulation. In 2015-07-20, JPEM went down by -0.005505289, VGT went up by 0.00442037, SCH went down by -0.001932799, XOM went down by -0.0102893, and USO went down by -0.0176574. 
Then we set $100,000 as our initial wealth and ran the bootstrap resampling 5000 times to estimate the 4-week (20 trading day) value at risk of each of your three portfolios at the 5% level. Each rows of the result of the simulation represent simulated future and each columns represent 15000 different simulated values of final wealth of this portfolio after 20 days. The histogram below shows the wealth distribution of this portfolio.
```{r}
# Sample a random return from the empirical joint distribution
# This simulates a random day
return.today = resample(all_returns, 1, orig.ids=FALSE)

# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth = 100000
sim1 = foreach(i=1:15000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
# each row is a simulated trajectory (simulated future)
# each column is a data 
head(sim1)
hist(sim1[,n_days], 25)
```
As we expected from the graph above, we are expecting to loss about 124.25 dollars with Value at Risk of 9322.883 dollars. This means that 5% of this simulated future has a loss worse than 9322.883 dollars and the rest (95%) are better than losing 9322.883 dollars. 
```{r}
# Profit/loss
mean(sim1[,n_days]) # 99875.75
mean(sim1[,n_days] - initial_wealth) #-124.2499
hist(sim1[,n_days]- initial_wealth, breaks=30) #

# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth, prob=0.05) # -9322.883 
```
### Portfolio 2 
Let's contruct our second portfolio. This time, we used the following 6 ETFs:
VNQ = Vanguard Real Estate Index Fund
CPER = United States Copper Index Fund
RWO = SPDR DJ Wilshire Global Real Estate ETF
ITB = iShares U.S. Home Construction ETF
XME = SPDR S&P Metals & Mining ETF
XLK = Technology Select Sector SPDR Fund

We chose these ETFs because we wanted to concentrate on real estate and metal market. So we chose real estate ETF (VNQ), global real estate ETF (RWO), buildings and construction ETF (ITB), copper ETF (CPER), metals and mining (XME), and technology ETF (XLK). We extracted data from May 22, 2008. The plots below show more correlation between each ETFs than that of the previous portfolio, except CPER which seems no correlation between all the other ETFs. 
```{r}
mystocks2 = c("VNQ", "CPER", "RWO", "ITB", "XME", "XLK")
getSymbols(mystocks2, from="2008-05-22")

for(ticker in mystocks2) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

# Combine all the returns in a matrix
all_returns2 = cbind(	ClCl(VNQa),
								ClCl(CPERa),
								ClCl(RWOa),
								ClCl(ITBa),
								ClCl(XMEa),
								ClCl(XLKa))
all_returns2 = as.matrix(na.omit(all_returns2))

# Compute the returns from the closing prices
pairs(all_returns2)
```
Here, we splitted the weights into 6 (0.166 per ETFs) and ran simulation using bootstrap resampling under the same condition as before. 
```{r}
# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth = 100000
sim2 = foreach(i=1:15000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.166, 0.166, 0.166, 0.166, 0.166, 0.166)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns2, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
# each row is a simulated trajectory (simulated future)
# each column is a data 
head(sim2)
hist(sim2[,n_days], 25)
```
Now we are getting profit of 303.8911 dollars with value at risk 8028.351 dollars. This means that 5% of this simulated future has a loss worse than 8028.351 dollars and the rest (95%) are better than losing 8028.351 dollars. 
```{r}
# Profit/loss
mean(sim2[,n_days]) # 100303.9
mean(sim2[,n_days] - initial_wealth) #303.8911
hist(sim2[,n_days]- initial_wealth, breaks=30) 

# 5% value at risk:
quantile(sim2[,n_days]- initial_wealth, prob=0.05) # -8028.351 
```
### Portfolio 3
Lastly, here is our thrid portfolio. This portfolio contains following 8 ETFs:
VHT = Vanguard Healthcare ETF
DBA = Invesco DB Agriculture Fund
PDBC = Invesco Optimum Yield Diversified Commodity Strategy No K-1 ETF
GNR = SPDR S&P Global Natural Resources ETF
IJK = iShares S&P MidCap 400 Growth ETF
PHO = Invesco Water Resources ETF
FMAT = Fidelity MSCI Materials Index ETF
XLP = Consumer Staples Select Sector SPDR Fund

For this portfolio, we chose ETFs that are related to natural material and biotech. 
When we did the same process as before and observed the plot below, we could see that the ETFs most likely have positive correlation between each other. Note that some pairs are showing almost neutral correlation each other (such as VHT and DBA, VHT and PDBC, DBA and GNR, etc...) 
```{r}
mystocks3 = c("VHT", "DBA", "PDBC", "GNR", "IJK", "PHO", "FMAT", "XLP")
getSymbols(mystocks3, from="2014-11-07")

for(ticker in mystocks3) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

# Combine all the returns in a matrix
all_returns3 = cbind(	ClCl(VHTa),
								ClCl(DBAa),
								ClCl(PDBCa),
								ClCl(GNRa),
								ClCl(IJKa),
								ClCl(PHOa),
								ClCl(FMATa),
								ClCl(XLPa))
all_returns3 = as.matrix(na.omit(all_returns3))

# Compute the returns from the closing prices
pairs(all_returns3)
```
Since we had 8 ETFs, we equally splitted the weights for each ETFs (0.125) and ran the simulation using bottstrap resampling 15000 times. 
```{r}
initial_wealth = 100000
sim3 = foreach(i=1:15000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns3, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
# each row is a simulated trajectory (simulated future)
# each column is a data 
head(sim3)
hist(sim3[,n_days], 25)
```
Our thrid portfolio shows mean of 100358.8 dollars, indicating 358.8479 dollars profit with value at risk 6844.874 dollars. This means that 5% of this simulated future has a loss worse than 6844.874 dollars and the rest (95%) are better than losing 6844.874 dollars. 
```{r}
# Profit/loss
mean(sim3[,n_days]) # 100358.8
mean(sim3[,n_days] - initial_wealth) #358.8479
hist(sim3[,n_days]- initial_wealth, breaks=30) 

# 5% value at risk:
quantile(sim3[,n_days]- initial_wealth, prob=0.05) # -6844.874 
```
To sum up, we could get these results from our three portfolios:
Portfolio 1 (5 random ETFs): loss = $124.2499, VaR = $9322.883 
Portfolio 2 (6 related ETFs): profit = $303.8911, VaR = $8028.351 
Portfolio 3 (8 related ETFs): profit = $358.8479, VaR = $6844.874 

In conclusion, throughout our three simulations utilzing bootstrap resampling technique, we discovered that choosing random (unrelated) ETFs is definitely not profitable choice. While some might claim that splitting ETFs into various unrelated assets of ETF is profitable way because of its wide range of ETFs, our simulation denies that notion. Rather, our simulations indicate that choosing related ETFs in fact gives better results and profit. Also note that the VaR has gotten low as we added more related ETFs. Therefore, we could conclude that inverting into related ETFs is the better way of getting profit with low VaR than investing into random ETFs.

## Market segmentation
### Data Cleaning
We divided columns by relevant interests. For example, we gathered sport columns like "sports_fandom", "sports_playing" and "personal training". This results more sorted data set by their relevant interests. 
```{r}
library(ggplot2)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)
library(tidyverse)

data <- read.csv("social_marketing.csv")
# reorder the columns
sns <- data[, -6]
sns$uncategorized <- data[, 6]

data2 <- read.csv("social_marketing.csv", row.names = 1)
sns_reorder <- data2[, c(1,2,4,29, 9,10,15,28, 18,8, 30,16, 32,27,23,14, 6,17,31, 20,13,24, 3,22,19, 21,33, 12,7,5, 25,11, 26, 34,35,36)]

```

Before we dive into data analysis, let's first see the total distribution across the interests. Note that "chatter" is the highest, followed by "photo_sharing". This is a straightforward information since most of posts in Twitter can be fall into these two categories. One interesting aspect is that "health_nutrition" is the highest except "chatter" and "photo_sharing". This roughly indicates that many of the brand's followers are interested in health and nutrient. Maybe that was the reason why they followed the brand NutrientH20, which our group guessed as nutrition brand as the name of the brand indicates. 
```{r}
par(mar=c(11,4,4,4))
barplot(colSums(sns_reorder), horiz=TRUE, xlim=c(0, 40000), main="Total Distribution Across the Interests", col=rainbow(36), cex.names=0.5, axis.lty=1, las=2)
```
### PCA
The picture below shows our group's definition of market segmentation. We divided each related interests into separate groups. For example, we gathered "family", "home_&_garden", "health_nutrition", and "parenting" as family-related group. We also gathered "travel", "eco", and "outdoor" as travel-related group. There are 13 clusters in total in our drawing. Note that this is mere a rough-draft of our entire process. We thought this drawing gives nice overview of our 36 interests in the dataset. 
![Rough Drawing of Market Segmentaion](/Users/macintosh/Documents/R studio/ourmarketsegmentation.jpeg)

In order to perform PCA, we first measured means of all interests and created a new data frame. Then we created a correlation matrix to see overall correlation between each interests in our data set. Although the matrix shows not many correlation, there are some strong positive correlation which indicates that we could reduce the dimensionality. Interestingly enough, we could observe that some of our "prediction" of above drawing actually are represented in below correlation matrix. For example, "politics" has strong correlation with "news" and "beauty" has strong correlation with fashion. However, there are some correlations that we did not expect: "food", "sports_fandom", "religion" and "parenting", and "travel" and "politics". Recall that this data was achieved from human annotators. Such unexpected correlation might be influenced by inevitable error by annotators. 
```{r}
sns_results = sns %>%
	group_by(X) %>% 
	summarize_all(mean) %>% 
  column_to_rownames(var="X")

# a look at the correlation matrix
head(cor(sns_results))

# looks a mess -- reorder the variables by hierarchical clustering
ggcorrplot::ggcorrplot(cor(sns_results), title="Correlation Plot", type="lower", hc.order = TRUE)
```
We performed PCA and created the variance plot of each PCs. Obviously, the first PC has the highest variance while others share somewhat similar behaviors.
```{r}
# Now look at PCA of the (average) survey responses.  
# This is a common way to treat survey data
PCAsns = prcomp(sns_results, scale=TRUE)

## variance plot
plot(PCAsns)
#summary(PCAsns)
```
We checked the number of PCs by creating an elbow plot. The elbow plot below shows the knee around 11 PCs. So we could narrow our 36 PCs down to 11 PCs. 
```{r}
# std dev of each PCs
std_dev <- PCAsns$sdev

# Variances of each PCs
pca_var <- std_dev^2
# check first 10 variance
pca_var[1:10]

# We want to find the components which explain the maximum variance.
# Higher variance retains more information 

# To compute the proportion of variance explained by each component, 
# we simply divide the variance by sum of total variance.

# proportion of variance explained
prop_var <- pca_var / sum(pca_var)
prop_var[1:10]

plot(prop_var, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")
```

```{r}
# create a tidy summary of the loadings
loadings_summary = PCAsns$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Interest')

loadings_summary %>%
  select(Interest, PC1) %>%
  arrange(desc(PC1))

loadings_summary %>%
  select(Interest, PC2) %>%
  arrange(desc(PC2))
```


### K Means Clustering
Before we perform K-Means Clustering, we scaled our PCA values and created an elbow plot for the right number of K. Among 20 Ks, the elbow plot shows a knee near when K equals 12. Interestingly enough, this was very close to what we roughly predicted when we drew the picture above. Although the correlation matrix showed some unexpected correlation, we thought this was interesting fact to note. 
```{r}
X <- as.data.frame(PCAsns$x[, 1:11])

sns_scaled = scale(X, center=TRUE, scale=TRUE)

k_grid = seq(2, 20, by=1)
SSE_grid = foreach(k = k_grid, .combine="c") %do%{
  cluster_k = kmeans(sns_scaled, k, nstart=50)
  cluster_k$tot.withinss
}

plot(k_grid, SSE_grid)
```
We then performed K-Means Clustering with 25 different random initializations. The table below shows the centers of the clusters. 
```{r}
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(sns_scaled,"scaled:center")
sigma = attr(sns_scaled,"scaled:scale")

# Run k-means with 6 clusters and 25 starts
clust1 = kmeans(sns_scaled, 13, nstart=25)
# 25 different random initialization and the result of the algorithm is the best of those 25 so it's like running a tournament of 25 different random

# What are the clusters?
clust1$center[1,]*sigma + mu
#clust1$center[2,]*sigma + mu
#clust1$center[4,]*sigma + mu
```
We also tried K Means++, but its sum of within-cluster sum of squares is actually higher and its between-cluster sum of squares is lower than regular K Means. So we decided to stick with regular K Means clustering as our model.  
```{r}
# Using kmeans++ initialization
clust2 = kmeanspp(sns_scaled, k=13, nstart=25)

qplot(chatter, health_nutrition, data=sns_reorder, color=factor(clust2$cluster))
qplot(college_uni, school, data=sns_reorder, color=factor(clust2$cluster))

# Compare versus within-cluster average distances from the first run
clust1$withinss
clust2$withinss
sum(clust1$withinss)
sum(clust2$withinss)
clust1$tot.withinss
clust2$tot.withinss
clust1$betweenss
clust2$betweenss
```

Now we plotted the result as shown below. All plots share similar behavior: It looks like other clusters are moving towards the cluster 1. Below is the plot of PC1 and PC2.
```{r}
# plot
qplot(PC1, PC2, data=X, color=factor(clust1$cluster))
```
Below is the plot of PC2 and PC3
```{r}
qplot(PC2, PC3, data=X, color=factor(clust1$cluster))
```
Below is the plot of PC3 and PC4
```{r}
qplot(PC3, PC4, data=X, color=factor(clust1$cluster))
```
```{r}
qplot(PC4, PC5, data=X, color=factor(clust1$cluster))
```

```{r}
for (i in seq(1:13)) {
  print(paste("Number of Cluster ", i, ": ", length(which(clust1$cluster == i))))
}
```

## Author Attribution
For this problem, we utilized "tm" library for reading our text data sets. The "readPlain" function reads in English texts so that we can work on it in our R studio enviroment. Then we loaded our C50train data set by "Sys.glob" function. This function allows us to load all the data we need inside the C50train folder. After that, we had 50 training article data (2500 in total). We then cleaned up the file names by using "strsplit" function so that our file names look like "AaronPressman106247newsML.txt". 
We used "VectorSource" function in order to create a vector source of our data. Then we used "Corpus" function so that we can make kind of a dataframe that contains 2500 articles. 
```{r}
install.packages('tm', repos="http://cran.us.r-project.org")
install.packages('e1071', repos="http://cran.us.r-project.org")
install.packages('gmodels', repos="http://cran.us.r-project.org")
library(tm) 
library(tidyverse)
library(slam)
library(proxy)
library(tidyr)
library(glmnet)
library(e1071) 
library(gmodels)
library(caret)


readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
						
## Test it on Adam Smith
#adam = readerPlain("/Users/macintosh/Documents/R studio/division_of_labor.txt")
#adam
#meta(adam)
#content(adam)

# load all 50 training articles (2500 total)
fileNames <- Sys.glob("/Users/macintosh/Documents/R studio/ReutersC50/C50train/*/*.txt")
articles_train = lapply(fileNames, readerPlain)
#fileNames

# Clean up the file names
mynames = fileNames %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist

# Rename the articles
#author_names <- c()

#for (i in seq(1, length(fileNames))){
#  name <- strsplit(fileNames[i], split = "/")[[1]][8]
#  author_names <- append(author_names, name)
#}
#mynames
names(articles_train) = mynames 

documents_raw = Corpus(VectorSource(articles_train))
```
For preprocessing and tokenization steps, we utilized "content_transformer" function to remove numbers, punctuation, excess white-spaces, and converting everything into lowercase. Then we used "tm_map" to map these into the corpus dataframe. 
```{r}
## Some pre-processing/tokenization steps.
## tm_map just maps some function to every document in the corpus
my_documents = documents_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))          # remove excess white-space
```
Then we removed stopwords. Among two built-in stopwords, we decided to use "en" which is just basic English stopwords. 
```{r}
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
```
We created a doc-term-matrix from the corpus. This matrix shows 99% sparsity which indicates that there are 99% of data which are zeros. 
```{r}
DTM_train = DocumentTermMatrix(my_documents)
DTM_train # some basic summary statistics
```
We explored some functions for the doc-term-matrix from the corpus such as "inspect", "findFreqTerms", and "findAssocs". We used word "genetic" that we used in our class for finding association terms. 
```{r}
inspect(DTM_train)

head(findFreqTerms(DTM_train, 50))

findAssocs(DTM_train, "genetic", .5)
```
As our final preprocessing step, we removed the terms that have low frequency (removing the "long tail"). We utilized "removeSparseTerms" to remove any sparse terms in out document-term matrix. We set the maximal allowed sparsity of 95%. Such reduced our terms into 802 terms instead of about 32571 terms.
Then we built TF IDF weights matrix as our final step for data analysis. 
```{r}
DTM_train = removeSparseTerms(DTM_train, 0.95)
DTM_train 

# construct TF IDF weights -- might be useful if we wanted to use these
# as features in a predictive model
tfidf_train = weightTfIdf(DTM_train)
```
## PCA 

Before we dive into the model, we decided to do PCA in order to reduce the dimensionality of our data set. We dropped all zeros in order to make our process smooth. The "prcomp" function automatically chose 785 (columns) rather than 2500 (rows) because it automatically chooses the lower number between the two. 
```{r}
# Now PCA on term frequencies
tfidf_matix = as.matrix(tfidf_train) # make it into a matrix since prcomp requires matrix format. 
summary(colSums(tfidf_matix))
scrub_cols = which(colSums(tfidf_matix) == 0) # drops all zeros
tfidf_matix = tfidf_matix[,-scrub_cols]

pca_train = prcomp(tfidf_matix, scale=TRUE) # since without "rank" parameter, it automatically select the lower value btw nrow and ncol. 
#summary(pca_train) 

# Look at the loadings
pca_train$rotation[order(abs(pca_train$rotation[,1]),decreasing=TRUE),1][1:25]
pca_train$rotation[order(abs(pca_train$rotation[,2]),decreasing=TRUE),2][1:25]

```
As expected, the graph below shows that the first component is the highest. However, the components after the first component are gradually decreasing. This is because our data set is so big that the components after the first component don't have much difference. In other words, the weight distribution of each components are  small compared to that of the example in our class which has total 50 components. 
```{r}
head(pca_train$x[,1:2])

# plot PCA components' variances 
plot(pca_train)
```
The graph below looks very messy. This is happening because we are trying to represent the whole data using only the first and second components. Since there are 785 components in total, the significance of the first and second components are comparably small. 
```{r}
plot(pca_train$x[,1:2], xlab="PCA 1 direction", ylab="PCA 2 direction", bty="n",
     type='n'); text(pca_train$x[,1:2], labels = 1:length(articles_train), cex=0.5)
# Let's check 2002 and 2036 -> looks like the article is about communications
content(articles_train[[2002]])
content(articles_train[[2036]])
```
In order to choose the number of components, we checked the variances of each components and created an elbow plot. Since the variance between 200 components and 785 components is small (we thought it is not worthy to take hundreds more components to improve small amount), we decided to choose 200 components. By using PCA, we could reduce the dimension into 25% of the entire components. 
```{r}
# std dev of each PCs
std_dev <- pca_train$sdev

# Variances of each PCs
pca_var <- std_dev^2
# check first 10 variance
pca_var[1:10]

# We want to find the components which explain the maximum variance.
# Higher variance retains more information 

# To compute the proportion of variance explained by each component, 
# we simply divide the variance by sum of total variance.

# proportion of variance explained
prop_var <- pca_var / sum(pca_var)
prop_var[1:10]

plot(prop_var, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")
```
## Naive Bayes Classification 
With 200 components that we achieved from previous PCA process, we used C50train data to build Naive Bayes Classification model. Note that we also added "pseudo-count" into our trainng data in order to avoid any zero probability in our C50test set. 
```{r}
author_names <- c()

for (i in seq(1, length(fileNames))){
  name <- strsplit(fileNames[i], split = "/")[[1]][8]
  author_names <- append(author_names, name)
}

pca_df <- as.data.frame(pca_train$x[,1:200])
pca_df <- cbind("Authors" = as.factor(author_names), pca_df)

x_nB <- pca_df[,2:201]
y_nB <- pca_df[,1]

tr = 0.8
n = nrow(pca_df)
d = ncol(pca_df)
train_set = sort(sample.int(n, floor(tr*n)))
test_set = setdiff(1:n, train_set)

X_train = x_nB + 1/d # smoothing
y_train = y_nB
#X_test = x_nB[test_set,]
#y_test = y_nB[test_set]

nb_model <- naiveBayes(X_train, y_train)
#nb_model
#yhat_train <- predict(nb_model, X_test) # 0.682 
#confusionMatrix(yhat_train, y_test)
```
We then loaded the real test data set (C50test). Since our model was fitted by the data set from data cleaning and PCA, we also did the same process on our C50test data set. One thing to note is that the graph below shows the first component of our C50test data set is little higher than our C50train data set while the second component is little lower than our C50train data set. And the rest components share similar behaviors. 
```{r}
# load the test data 
fileNamesTest <- Sys.glob("/Users/macintosh/Documents/R studio/ReutersC50/C50test/*/*.txt")
articles_Test = lapply(fileNamesTest, readerPlain)

mynamesTest = fileNamesTest %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist

names(articles_Test) = mynamesTest 

documents_raw_Test = Corpus(VectorSource(articles_Test))

my_documents_Test = documents_raw_Test %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))          # remove excess white-space
 
my_documents_Test = tm_map(my_documents_Test, content_transformer(removeWords), stopwords("en"))

DTM_test = DocumentTermMatrix(my_documents_Test)
DTM_test = removeSparseTerms(DTM_test, 0.95)
tfidf_test = weightTfIdf(DTM_test)   

# Now PCA on term frequencies
tfidf_test_matix = as.matrix(tfidf_test) # make it into a matrix since prcomp requires matrix format. 
summary(colSums(tfidf_test_matix))
scrub_cols = which(colSums(tfidf_test_matix) == 0) # drops all zeros
tfidf_test_matix = tfidf_test_matix[,-scrub_cols]

pca_test = prcomp(tfidf_test_matix, scale=TRUE)
plot(pca_test)
```
And we tested our model with C50test data. We also added "pseudo-count" as before. We utilized "confusionMatrix" from "caret" library in order to  observe our model's accuracy score. We got 0.8788 as our accuracy score. It is quite impressive that Naive Bayes Classification's performance on text classification. 
```{r}
pca_test_df <- as.data.frame(pca_test$x[,1:200])
pca_test_df <- cbind("Authors" = as.factor(author_names), pca_test_df)

x_test_nB <- pca_test_df[,2:201]
y_test_nB <- pca_test_df[,1]

#tr = 0.8
#n = nrow(pca_df)
#d = ncol(pca_df)
#train_set = sort(sample.int(n, floor(tr*n)))
#test_set = setdiff(1:n, train_set)

X_test = x_nB + 1/d # smoothing
y_test = y_nB
#X_test = x_nB[test_set,]
#y_test = y_nB[test_set]

# prediction 
yhat <- predict(nb_model, X_test) # 0.8788  
confusionMatrix(yhat, y_test) 
```



## Association rule mining

Since our data file is text file, we utilized "read.transactions" function from "arule" library to convert our text data into transaction data with dropping duplicates at the same time. As a result, we now have 9835 transactions (rows) with 169 unique food names (columns). 
```{r}
if (!require("arules")) install.packages('arules', repos="http://cran.us.r-project.org")
library(arules)
library(tidyverse)
library(arulesViz)

# load the data using arule
transactions <- arules::read.transactions(
  file="groceries.txt",
  format = c("basket"),
  sep = ",",
  cols =NULL,
  rm.duplicates = 1,
  skip = 0
)

#check 
head(transactions@itemInfo$labels)

print(paste("There are ", transactions@data@Dim[2], "transactions with ", transactions@data@Dim[1], "items."))
```
Among those 169 food names, the most frequent items are "whole milk", "other vegetables", "rolls/buns", "soda", and "yogurt". 
```{r}
summary(transactions)
```
The bar graph below shows the items that have frequency above 10% support. As we expected, we got "whole milk" as the highest, followed by "other vegetables" and "rolls/buns". 
```{r}
itemFrequencyPlot(transactions, support = 0.1, main = "item frequency plot above support 10%")
```
The graph below shows top 30 items with highest item frequency. 
```{r}
itemFrequencyPlot(transactions, topN=30, main = "Top 30 Item Frequency")
```
We ran the "apriori" function. When we inspected, we could observe that "bottled water", "tropical fruit", "root vegetables", "soda", "yogurt", "rolls/buns", "other vegetables" and "whole milk" have empty Left Hand Side (LHS). This means that these items are so popular items that these item is most likely to be purchased no matter what conditional probability of LHS. It is also worthy to note that the lift values of these popular items are 1. 
```{r}
# Run apriori
itemrules = apriori(transactions, parameter=list(support=.005, confidence=.1, maxlen=5))

head(inspect(itemrules))
```
Since there are too many rules, we chose lift threshold as 1 and confidence threshold as 0.5. The below table shows the Left Hand Side (antecedent) and Right Hand Side (consequent), support, confidence, lift and etc...
```{r}
head(inspect(subset(itemrules, subset=lift > 1 & confidence > 0.5)))
```
The graph below plots the rules of our items data with 3 varaibles: x-axis as support, y-axis as confidence, and depth of each scatter pointes as lift. 
```{r}
plot(itemrules)
```
Here, we utilzed two-key plot which colors the scatter plot by its order(size).
```{r}
plot(itemrules, method='two-key plot')
```
Here, we swapped lift and confidence so that the confidence is now the depth and lift is y-axis. 
```{r}
# can swap the axes and color scales
plot(itemrules, measure = c("support", "lift"), shading = "confidence")
```

By observing the plots above, we could test out different subsets of the rule. Here, we tried to find the case where LHS and RHS have strong relationship between them. Through the graph right above, we could estimate our threshold of lift (2) and confidence (0.45). Below table shows the items that have strong relationship between them. Most of them are quite obvious which indicates that our model is working fine.
```{r}
head(inspect(subset(itemrules, lift > 2 & confidence > 0.45)))
```
The graph below shows graph for 100 rules. The transparent circles represent support, and the circle's color represents lift (more red = high lift). 
```{r}
sub1 = subset(itemrules, subset=confidence > 0.5 & support > 0.005)
summary(sub1)
plot(head(sub1, 100, by='lift'), method='graph')
saveAsGraph(head(itemrules, n = 1000, by = "lift"), file = "itemrules.graphml")
```
Below graphs are exported from the visualization program "Gephi". The first graph is a result from the layout called "Fruchterman Reingold" which represents the whoel association rules in a circular way (heavier as close to the center) and the second one is the result from the layout called "Label Layout" which spreads out the rules by the frequency of items (heavier as far away from the center). 

![FruchtermanReingold](/Users/macintosh/Documents/R studio/Gephi-Fruchterman Reingold.png)
![Label-Layout](/Users/macintosh/Documents/R studio/Gephi-Label Layout.png)